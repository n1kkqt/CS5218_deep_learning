{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collection of the core mathematical operators used throughout the code base.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# ## Task 0.1\n",
    "\n",
    "# Implementation of a prelude of elementary functions.\n",
    "\n",
    "\n",
    "def mul(x, y):\n",
    "    \":math:`f(x, y) = x * y`\"\n",
    "    return float(x * y)\n",
    "\n",
    "\n",
    "def id(x):\n",
    "    \":math:`f(x) = x`\"\n",
    "    return x\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    \":math:`f(x, y) = x + y`\"\n",
    "    return float(x + y)\n",
    "\n",
    "\n",
    "def neg(x):\n",
    "    \":math:`f(x) = -x`\"\n",
    "    return -float(x)\n",
    "\n",
    "\n",
    "def lt(x, y):\n",
    "    \":math:`f(x) =` 1.0 if x is less than y else 0.0\"\n",
    "    if x < y:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def eq(x, y):\n",
    "    \":math:`f(x) =` 1.0 if x is equal to y else 0.0\"\n",
    "    if x == y:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def max(x, y):\n",
    "    \":math:`f(x) =` x if x is greater than y else y\"\n",
    "    if x > y:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(y)\n",
    "\n",
    "\n",
    "def is_close(x, y):\n",
    "    \":math:`f(x) = |x - y| < 1e-2` \"\n",
    "    return abs(x - y) < 1e-2\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    r\"\"\"\n",
    "    :math:`f(x) =  \\frac{1.0}{(1.0 + e^{-x})}`\n",
    "\n",
    "    (See `<https://en.wikipedia.org/wiki/Sigmoid_function>`_ .)\n",
    "\n",
    "    Calculate as\n",
    "\n",
    "    :math:`f(x) =  \\frac{1.0}{(1.0 + e^{-x})}` if x >=0 else :math:`\\frac{e^x}{(1.0 + e^{x})}`\n",
    "\n",
    "    for stability.\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float : sigmoid value\n",
    "    \"\"\"\n",
    "    if x >= 0:\n",
    "        return 1.0 / (1.0 + math.exp(-x))\n",
    "    else:\n",
    "        return math.exp(x) / (1.0 + math.exp(x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    :math:`f(x) =` x if x is greater than 0, else 0\n",
    "\n",
    "    (See `<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>`_ .)\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float : relu value\n",
    "    \"\"\"\n",
    "    return max(0.0, float(x))\n",
    "\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    \":math:`f(x) = log(x)`\"\n",
    "    return math.log(x + EPS)\n",
    "\n",
    "\n",
    "def exp(x):\n",
    "    \":math:`f(x) = e^{x}`\"\n",
    "    return math.exp(x)\n",
    "\n",
    "\n",
    "def log_back(x, d):\n",
    "    r\"If :math:`f = log` as above, compute d :math:`d \\times f'(x)`\"\n",
    "    return d / (x + EPS)\n",
    "\n",
    "\n",
    "def inv(x):\n",
    "    \":math:`f(x) = 1/x`\"\n",
    "    return 1.0 / x\n",
    "\n",
    "\n",
    "def inv_back(x, d):\n",
    "    r\"If :math:`f(x) = 1/x` compute d :math:`d \\times f'(x)`\"\n",
    "    return -d / (x ** 2 + EPS)\n",
    "\n",
    "\n",
    "def relu_back(x, d):\n",
    "    r\"If :math:`f = relu` compute d :math:`d \\times f'(x)`\"\n",
    "    if x > 0:\n",
    "        return float(d)\n",
    "    if x < 0:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# ## Task 0.3\n",
    "\n",
    "# Small library of elementary higher-order functions for practice.\n",
    "\n",
    "\n",
    "def map(fn):\n",
    "    \"\"\"\n",
    "    Higher-order map.\n",
    "\n",
    "    .. image:: figs/Ops/maplist.png\n",
    "\n",
    "\n",
    "    See `<https://en.wikipedia.org/wiki/Map_(higher-order_function)>`_\n",
    "\n",
    "    Args:\n",
    "        fn (one-arg function): Function from one value to one value.\n",
    "\n",
    "    Returns:\n",
    "        function : A function that takes a list, applies `fn` to each element, and returns a\n",
    "        new list\n",
    "    \"\"\"\n",
    "    return lambda ls: [fn(el) for el in ls]\n",
    "\n",
    "\n",
    "def negList(ls):\n",
    "    \"Use :func:`map` and :func:`neg` to negate each element in `ls`\"\n",
    "    return map(neg)(ls)\n",
    "\n",
    "\n",
    "def zipWith(fn):\n",
    "    \"\"\"\n",
    "    Higher-order zipwith (or map2).\n",
    "\n",
    "    .. image:: figs/Ops/ziplist.png\n",
    "\n",
    "    See `<https://en.wikipedia.org/wiki/Map_(higher-order_function)>`_\n",
    "\n",
    "    Args:\n",
    "        fn (two-arg function): combine two values\n",
    "\n",
    "    Returns:\n",
    "        function : takes two equally sized lists `ls1` and `ls2`, produce a new list by\n",
    "        applying fn(x, y) on each pair of elements.\n",
    "\n",
    "    \"\"\"\n",
    "    return lambda ls1, ls2: [fn(el1, el2) for el1, el2 in zip(ls1, ls2)]\n",
    "\n",
    "\n",
    "def addLists(ls1, ls2):\n",
    "    \"Add the elements of `ls1` and `ls2` using :func:`zipWith` and :func:`add`\"\n",
    "    return zipWith(add)(ls1, ls2)\n",
    "\n",
    "\n",
    "def reduce(fn, start):\n",
    "    r\"\"\"\n",
    "    Higher-order reduce.\n",
    "\n",
    "    .. image:: figs/Ops/reducelist.png\n",
    "\n",
    "\n",
    "    Args:\n",
    "        fn (two-arg function): combine two values\n",
    "        start (float): start value :math:`x_0`\n",
    "\n",
    "    Returns:\n",
    "        function : function that takes a list `ls` of elements\n",
    "        :math:`x_1 \\ldots x_n` and computes the reduction :math:`fn(x_3, fn(x_2,\n",
    "        fn(x_1, x_0)))`\n",
    "    \"\"\"\n",
    "\n",
    "    def fun(ls):\n",
    "        total = start\n",
    "        for el in ls:\n",
    "            total = fn(total, el)\n",
    "        return total\n",
    "\n",
    "    return fun\n",
    "\n",
    "\n",
    "def sum(ls):\n",
    "    \"Sum up a list using :func:`reduce` and :func:`add`.\"\n",
    "    return reduce(add, 0)(ls)\n",
    "\n",
    "\n",
    "def prod(ls):\n",
    "    \"Product of a list using :func:`reduce` and :func:`mul`.\"\n",
    "    return reduce(mul, 1)(ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        history (:class:`History` or None) : the Function calls that created this variable or None if constant\n",
    "        derivative (variable type): the derivative with respect to this variable\n",
    "        grad (variable type) : alias for derivative, used for tensors\n",
    "        name (string) : a globally unique name of the variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history, name=None):\n",
    "        global variable_count\n",
    "        assert history is None or isinstance(history, History), history\n",
    "\n",
    "        self.history = history\n",
    "        self._derivative = None\n",
    "\n",
    "        # This is a bit simplistic, but make things easier.\n",
    "        variable_count += 1\n",
    "        self.unique_id = \"Variable\" + str(variable_count)\n",
    "\n",
    "        # For debugging can have a name.\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = self.unique_id\n",
    "        self.used = 0\n",
    "\n",
    "    def requires_grad_(self, val):\n",
    "        \"\"\"\n",
    "        Set the requires_grad flag to `val` on variable.\n",
    "\n",
    "        Ensures that operations on this variable will trigger\n",
    "        backpropagation.\n",
    "\n",
    "        Args:\n",
    "            val (bool): whether to require grad\n",
    "        \"\"\"\n",
    "        self.history = History()\n",
    "\n",
    "    def backward(self, d_output=None):\n",
    "        \"\"\"\n",
    "        Calls autodiff to fill in the derivatives for the history of this object.\n",
    "\n",
    "        Args:\n",
    "            d_output (number, opt): starting derivative to backpropagate through the model\n",
    "                                   (typically left out, and assumed to be 1.0).\n",
    "        \"\"\"\n",
    "        if d_output is None:\n",
    "            d_output = 1.0\n",
    "        backpropagate(self, d_output)\n",
    "\n",
    "    @property\n",
    "    def derivative(self):\n",
    "        return self._derivative\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"True if this variable created by the user (no `last_fn`)\"\n",
    "        return self.history.last_fn is None\n",
    "\n",
    "    def accumulate_derivative(self, val):\n",
    "        \"\"\"\n",
    "        Add `val` to the the derivative accumulated on this variable.\n",
    "        Should only be called during autodifferentiation on leaf variables.\n",
    "\n",
    "        Args:\n",
    "            val (number): value to be accumulated\n",
    "        \"\"\"\n",
    "        assert self.is_leaf(), \"Only leaf variables can have derivatives.\"\n",
    "        if self._derivative is None:\n",
    "            self._derivative = self.zeros()\n",
    "        self._derivative += val\n",
    "\n",
    "    def zero_derivative_(self):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        Reset the derivative on this variable.\n",
    "        \"\"\"\n",
    "        self._derivative = self.zeros()\n",
    "\n",
    "    def zero_grad_(self):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        Reset the derivative on this variable.\n",
    "        \"\"\"\n",
    "        self.zero_derivative_()\n",
    "\n",
    "    def expand(self, x):\n",
    "        \"Placeholder for tensor variables\"\n",
    "        return x\n",
    "\n",
    "    # Helper functions for children classes.\n",
    "\n",
    "    def __radd__(self, b):\n",
    "        return self + b\n",
    "\n",
    "    def __rmul__(self, b):\n",
    "        return self * b\n",
    "\n",
    "    def zeros(self):\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Some helper functions for handling optional tuples.\n",
    "\n",
    "\n",
    "def wrap_tuple(x):\n",
    "    \"Turn a possible value into a tuple\"\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x,)\n",
    "\n",
    "\n",
    "def unwrap_tuple(x):\n",
    "    \"Turn a singleton tuple into a value\"\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Classes for Functions.\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"\n",
    "    Context class is used by `Function` to store information during the forward pass.\n",
    "\n",
    "    Attributes:\n",
    "        no_grad (bool) : do not save gradient information\n",
    "        saved_values (tuple) : tuple of values saved for backward pass\n",
    "        saved_tensors (tuple) : alias for saved_values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, no_grad=False):\n",
    "        self._saved_values = None\n",
    "        self.no_grad = no_grad\n",
    "\n",
    "    def save_for_backward(self, *values):\n",
    "        \"\"\"\n",
    "        Store the given `values` if they need to be used during backpropagation.\n",
    "\n",
    "        Args:\n",
    "            values (list of values) : values to save for backward\n",
    "        \"\"\"\n",
    "        if self.no_grad:\n",
    "            return\n",
    "        self._saved_values = values\n",
    "\n",
    "    @property\n",
    "    def saved_values(self):\n",
    "        assert not self.no_grad, \"Doesn't require grad\"\n",
    "        assert self._saved_values is not None, \"Did you forget to save values?\"\n",
    "        return unwrap_tuple(self._saved_values)\n",
    "\n",
    "    @property\n",
    "    def saved_tensors(self):  # pragma: no cover\n",
    "        return self.saved_values\n",
    "\n",
    "\n",
    "class History:\n",
    "    \"\"\"\n",
    "    `History` stores the history of `Function` operations that was\n",
    "    used to construct the current Variable.\n",
    "\n",
    "    Attributes:\n",
    "        last_fn (:class:`FunctionBase`) : The last Function that was called.\n",
    "        ctx (:class:`Context`): The context for that Function.\n",
    "        inputs (list of inputs) : The inputs that were given when `last_fn.forward` was called.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, last_fn=None, ctx=None, inputs=None):\n",
    "        self.last_fn = last_fn\n",
    "        self.ctx = ctx\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backprop_step(self, d_output):\n",
    "        \"\"\"\n",
    "        Run one step of backpropagation by calling chain rule.\n",
    "\n",
    "        Args:\n",
    "            d_output : a derivative with respect to this variable\n",
    "\n",
    "        Returns:\n",
    "            list of numbers : a derivative with respect to `inputs`\n",
    "        \"\"\"\n",
    "        # TODO: Implement for Task 1.4.\n",
    "        raise NotImplementedError(\"Need to implement for Task 1.4\")\n",
    "\n",
    "\n",
    "class FunctionBase:\n",
    "    \"\"\"\n",
    "    A function that can act on :class:`Variable` arguments to\n",
    "    produce a :class:`Variable` output, while tracking the internal history.\n",
    "\n",
    "    Call by :func:`FunctionBase.apply`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def variable(raw, history):\n",
    "        # Implement by children class.\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *vals):\n",
    "        \"\"\"\n",
    "        Apply is called by the user to run the Function.\n",
    "        Internally it does three things:\n",
    "\n",
    "        a) Creates a Context for the function call.\n",
    "        b) Calls forward to run the function.\n",
    "        c) Attaches the Context to the History of the new variable.\n",
    "\n",
    "        There is a bit of internal complexity in our implementation\n",
    "        to handle both scalars and tensors.\n",
    "\n",
    "        Args:\n",
    "            vals (list of Variables or constants) : The arguments to forward\n",
    "\n",
    "        Returns:\n",
    "            `Variable` : The new variable produced\n",
    "\n",
    "        \"\"\"\n",
    "        # Go through the variables to see if any needs grad.\n",
    "        raw_vals = []\n",
    "        need_grad = False\n",
    "        for v in vals:\n",
    "            if isinstance(v, Variable):\n",
    "                if v.history is not None:\n",
    "                    need_grad = True\n",
    "                v.used += 1\n",
    "                raw_vals.append(v.get_data())\n",
    "            else:\n",
    "                raw_vals.append(v)\n",
    "\n",
    "        # Create the context.\n",
    "        ctx = Context(not need_grad)\n",
    "\n",
    "        # Call forward with the variables.\n",
    "        c = cls.forward(ctx, *raw_vals)\n",
    "        assert isinstance(c, cls.data_type), \"Expected return typ %s got %s\" % (\n",
    "            cls.data_type,\n",
    "            type(c),\n",
    "        )\n",
    "\n",
    "        # Create a new variable from the result with a new history.\n",
    "        back = None\n",
    "        if need_grad:\n",
    "            back = History(cls, ctx, vals)\n",
    "        return cls.variable(cls.data(c), back)\n",
    "\n",
    "    @classmethod\n",
    "    def chain_rule(cls, ctx, inputs, d_output):\n",
    "        \"\"\"\n",
    "        Implement the derivative chain-rule.\n",
    "\n",
    "        Args:\n",
    "            ctx (:class:`Context`) : The context from running forward\n",
    "            inputs (list of args) : The args that were passed to :func:`FunctionBase.apply` (e.g. :math:`x, y`)\n",
    "            d_output (number) : The `d_output` value in the chain rule.\n",
    "\n",
    "        Returns:\n",
    "            list of (`Variable`, number) : A list of non-constant variables with their derivatives\n",
    "            (see `is_constant` to remove unneeded variables)\n",
    "\n",
    "        \"\"\"\n",
    "        # Tip: Note when implementing this function that\n",
    "        # cls.backward may return either a value or a tuple.\n",
    "        # TODO: Implement for Task 1.3.\n",
    "        cls_out = wrap_tuple(cls.backward(ctx, d_output))\n",
    "\n",
    "        return [var.backward(ctx, d_output) for var in inputs if is_constant(var)]\n",
    "\n",
    "\n",
    "# Algorithms for backpropagation\n",
    "\n",
    "\n",
    "def is_constant(val):\n",
    "    return not isinstance(val, Variable) or val.history is None\n",
    "\n",
    "\n",
    "def topological_sort(variable):\n",
    "    \"\"\"\n",
    "    Computes the topological order of the computation graph.\n",
    "\n",
    "    Args:\n",
    "        variable (:class:`Variable`): The right-most variable\n",
    "\n",
    "    Returns:\n",
    "        list of Variables : Non-constant Variables in topological order\n",
    "                            starting from the right.\n",
    "    \"\"\"\n",
    "    # TODO: Implement for Task 1.4.\n",
    "    raise NotImplementedError(\"Need to implement for Task 1.4\")\n",
    "\n",
    "\n",
    "def backpropagate(variable, deriv):\n",
    "    \"\"\"\n",
    "    Runs backpropagation on the computation graph in order to\n",
    "    compute derivatives for the leave nodes.\n",
    "\n",
    "    See :doc:`backpropagate` for details on the algorithm.\n",
    "\n",
    "    Args:\n",
    "        variable (:class:`Variable`): The right-most variable\n",
    "        deriv (number) : Its derivative that we want to propagate backward to the leaves.\n",
    "\n",
    "    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.\n",
    "    \"\"\"\n",
    "    # TODO: Implement for Task 1.4.\n",
    "    raise NotImplementedError(\"Need to implement for Task 1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_difference(f, *vals, arg=0, epsilon=1e-6):\n",
    "    r\"\"\"\n",
    "    Computes an approximation to the derivative of `f` with respect to one arg.\n",
    "\n",
    "    See :doc:`derivative` or https://en.wikipedia.org/wiki/Finite_difference for more details.\n",
    "\n",
    "    Args:\n",
    "        f : arbitrary function from n-scalar args to one value\n",
    "        *vals (list of floats): n-float values :math:`x_0 \\ldots x_{n-1}`\n",
    "        arg (int): the number :math:`i` of the arg to compute the derivative\n",
    "        epsilon (float): a small constant\n",
    "\n",
    "    Returns:\n",
    "        float : An approximation of :math:`f'_i(x_0, \\ldots, x_{n-1})`\n",
    "    \"\"\"\n",
    "    x_start, x_end = list(vals), list(vals)\n",
    "    x_start[arg] = x_start[arg] - epsilon\n",
    "    x_end[arg] = x_end[arg] + epsilon\n",
    "\n",
    "    return 0.5 * (f(*x_end) - f(*x_start)) / epsilon\n",
    "\n",
    "\n",
    "# ## Task 1.2 and 1.4\n",
    "# Scalar Forward and Backward\n",
    "\n",
    "\n",
    "class Scalar(Variable):\n",
    "    \"\"\"\n",
    "    A reimplementation of scalar values for autodifferentiation\n",
    "    tracking.  Scalar Variables behave as close as possible to standard\n",
    "    Python numbers while also tracking the operations that led to the\n",
    "    number's creation. They can only be manipulated by\n",
    "    :class:`ScalarFunction`.\n",
    "\n",
    "    Attributes:\n",
    "        data (float): The wrapped scalar value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v, back=History(), name=None):\n",
    "        super().__init__(back, name=name)\n",
    "        self.data = float(v)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Scalar(%f)\" % self.data\n",
    "\n",
    "    def __mul__(self, b):\n",
    "        return Mul.apply(self, b)\n",
    "\n",
    "    def __truediv__(self, b):\n",
    "        return Mul.apply(self, Inv.apply(b))\n",
    "\n",
    "    def __rtruediv__(self, b):\n",
    "        return Mul.apply(b, Inv.apply(self))\n",
    "\n",
    "    def __add__(self, b):\n",
    "        return Add.apply(self, b)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.data)\n",
    "\n",
    "    def __lt__(self, b):\n",
    "        return LT.apply(self, b)\n",
    "\n",
    "    def __gt__(self, b):\n",
    "        return LT.apply(b, self)\n",
    "\n",
    "    def __eq__(self, b):\n",
    "        return EQ.apply(self, b)\n",
    "\n",
    "    def __sub__(self, b):\n",
    "        return Add.apply(self, Neg.apply(b))\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Neg.apply(self)\n",
    "\n",
    "    def log(self):\n",
    "        return Log.apply(self)\n",
    "\n",
    "    def exp(self):\n",
    "        return Exp.apply(self)\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return Sigmoid.apply(self)\n",
    "\n",
    "    def relu(self):\n",
    "        return ReLU.apply(self)\n",
    "\n",
    "    def get_data(self):\n",
    "        \"Returns the raw float value\"\n",
    "        return self.data\n",
    "\n",
    "\n",
    "class ScalarFunction(FunctionBase):\n",
    "    \"\"\"\n",
    "    A wrapper for a mathematical function that processes and produces\n",
    "    Scalar variables.\n",
    "\n",
    "    This is a static class and is never instantiated. We use `class`\n",
    "    here to group together the `forward` and `backward` code.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs):\n",
    "        r\"\"\"\n",
    "        Forward call, compute :math:`f(x_0 \\ldots x_{n-1})`.\n",
    "\n",
    "        Args:\n",
    "            ctx (:class:`Context`): A container object to save\n",
    "                                    any information that may be needed\n",
    "                                    for the call to backward.\n",
    "            *inputs (list of floats): n-float values :math:`x_0 \\ldots x_{n-1}`.\n",
    "\n",
    "        Should return float the computation of the function :math:`f`.\n",
    "        \"\"\"\n",
    "        pass  # pragma: no cover\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_out):\n",
    "        r\"\"\"\n",
    "        Backward call, computes :math:`f'_{x_i}(x_0 \\ldots x_{n-1}) \\times d_{out}`.\n",
    "\n",
    "        Args:\n",
    "            ctx (Context): A container object holding any information saved during in the corresponding `forward` call.\n",
    "            d_out (float): :math:`d_out` term in the chain rule.\n",
    "\n",
    "        Should return the computation of the derivative function\n",
    "        :math:`f'_{x_i}` for each input :math:`x_i` times `d_out`.\n",
    "\n",
    "        \"\"\"\n",
    "        pass  # pragma: no cover\n",
    "\n",
    "    # Checks.\n",
    "    variable = Scalar\n",
    "    data_type = float\n",
    "\n",
    "    @staticmethod\n",
    "    def data(a):\n",
    "        return a\n",
    "\n",
    "\n",
    "# Examples\n",
    "class Add(ScalarFunction):\n",
    "    \"Addition function :math:`f(x, y) = x + y`\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        return operators.add(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        return d_output, d_output\n",
    "\n",
    "\n",
    "class Log(ScalarFunction):\n",
    "    \"Log function :math:`f(x) = log(x)`\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.log(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a = ctx.saved_values\n",
    "        return operators.log_back(a, d_output)\n",
    "\n",
    "\n",
    "# To implement.\n",
    "\n",
    "\n",
    "class Mul(ScalarFunction):\n",
    "    \"Multiplication function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return operators.mul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a, b = ctx.saved_values\n",
    "        return operators.mul(a, d_output), operators.mul(b, d_output)\n",
    "\n",
    "\n",
    "class Inv(ScalarFunction):\n",
    "    \"Inverse function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.inv(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a = ctx.saved_values\n",
    "        return operators.inv_back(a, d_output)\n",
    "\n",
    "\n",
    "class Neg(ScalarFunction):\n",
    "    \"Negation function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.neg(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        return operators.neg(d_output)\n",
    "\n",
    "\n",
    "class Sigmoid(ScalarFunction):\n",
    "    \"Sigmoid function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.sigmoid(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a = ctx.saved_values\n",
    "        sig_val = operators.sigmoid(a, d_output)\n",
    "        return operators.mul(d_output, operators.mul(sig_val, (1 - sig_val)))\n",
    "\n",
    "\n",
    "class ReLU(ScalarFunction):\n",
    "    \"ReLU function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.relu(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a = ctx.saved_values\n",
    "        return operators.relu_back(a, d_output)\n",
    "\n",
    "\n",
    "class Exp(ScalarFunction):\n",
    "    \"Exp function\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        ctx.save_for_backward(a)\n",
    "        return operators.exp(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        a = ctx.saved_values\n",
    "        return operators.mul(d_output, operators.exp(a))\n",
    "\n",
    "\n",
    "class LT(ScalarFunction):\n",
    "    \"Less-than function :math:`f(x) =` 1.0 if x is less than y else 0.0\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return operators.lt(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        return 0.0, 0.0\n",
    "\n",
    "\n",
    "class EQ(ScalarFunction):\n",
    "    \"Equal function :math:`f(x) =` 1.0 if x is equal to y else 0.0\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return operators.eq(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        return 0.0, 0.0\n",
    "\n",
    "\n",
    "def derivative_check(f, *scalars):\n",
    "    \"\"\"\n",
    "    Checks that autodiff works on a python function.\n",
    "    Asserts False if derivative is incorrect.\n",
    "\n",
    "    Parameters:\n",
    "        f (function) : function from n-scalars to 1-scalar.\n",
    "        *scalars (list of :class:`Scalar`) : n input scalar values.\n",
    "    \"\"\"\n",
    "    for x in scalars:\n",
    "        x.requires_grad_(True)\n",
    "    out = f(*scalars)\n",
    "    out.backward()\n",
    "\n",
    "    vals = [v for v in scalars]\n",
    "    err_msg = \"\"\"\n",
    "Derivative check at arguments f(%s) and received derivative f'=%f for argument %d,\n",
    "but was expecting derivative f'=%f from central difference.\"\"\"\n",
    "    for i, x in enumerate(scalars):\n",
    "        check = central_difference(f, *vals, arg=i)\n",
    "        print(str([x.data for x in scalars]), x.derivative, i, check)\n",
    "        np.testing.assert_allclose(\n",
    "            x.derivative,\n",
    "            check.data,\n",
    "            1e-2,\n",
    "            1e-2,\n",
    "            err_msg=err_msg\n",
    "            % (str([x.data for x in scalars]), x.derivative, i, check.data),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function1(ScalarFunction):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        \":math:`f(x, y) = x + y + 10`\"\n",
    "        return x + y + 10\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        \"Derivatives are :math:`f'_x(x, y) = 1` and :math:`f'_y(x, y) = 1`\"\n",
    "        return d_output, d_output\n",
    "\n",
    "\n",
    "class Function2(ScalarFunction):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        \":math:`f(x, y) = x \\timex y + x`\"\n",
    "        ctx.save_for_backward(x, y)\n",
    "        return x * y + x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d_output):\n",
    "        \"Derivatives are :math:`f'_x(x, y) = y + 1` and :math:`f'_y(x, y) = x`\"\n",
    "        x, y = ctx.saved_values\n",
    "        return d_output * (y + 1), d_output * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tuple(x):\n",
    "    \"Turn a possible value into a tuple\"\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_constant(val):\n",
    "    return not isinstance(val, Variable) or val.history is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "var0 = Scalar(0)\n",
    "var1 = Function1.apply(0, var0)\n",
    "var2 = Function1.apply(0, var1)\n",
    "var3 = Function1.apply(0, var1)\n",
    "var4 = Function1.apply(var2, var3)\n",
    "#var4.backward(d_output=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(variable):\n",
    "    \"\"\"\n",
    "    Computes the topological order of the computation graph.\n",
    "\n",
    "    Args:\n",
    "        variable (:class:`Variable`): The right-most variable\n",
    "\n",
    "    Returns:\n",
    "        list of Variables : Non-constant Variables in topological order\n",
    "                            starting from the right.\n",
    "    \"\"\"\n",
    "    visited = []\n",
    "    visited_id = set()\n",
    "\n",
    "    def depth_first_search(current_node, visited, visited_id):\n",
    "        if current_node.unique_id in visited_id or is_constant(current_node):\n",
    "            return\n",
    "\n",
    "        nodes = current_node.history.inputs\n",
    "        if nodes is not None:\n",
    "            for node in nodes:\n",
    "                if not is_constant(node):\n",
    "                    depth_first_search(node, visited, visited_id)\n",
    "\n",
    "        visited.append(current_node)\n",
    "        visited_id.update([current_node.unique_id])\n",
    "\n",
    "    depth_first_search(variable, visited, visited_id)\n",
    "\n",
    "    return visited[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "var0.accumulate_derivative(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1801fca65b20b93fdd72d32a9e43ea8db97507df3ee1123ff7f0cb38f38992"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
